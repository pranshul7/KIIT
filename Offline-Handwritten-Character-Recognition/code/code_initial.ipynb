{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the train and test folders\n",
    "train_path = 'C:/A/code/VSC/python/test/cropped_DevanagariHandwrittenCharacterDataset/Train/'\n",
    "test_path = 'C:/A/code/VSC/python/test/cropped_DevanagariHandwrittenCharacterDataset/Test/'\n",
    "\n",
    "# Define a function to load and preprocess the images\n",
    "def load_images(path):\n",
    "    # Create empty lists to store the images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each folder (which represents a class) in the path\n",
    "    for folder in os.listdir(path):\n",
    "        # Get the path to the folder\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        # Loop through each image in the folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            # Get the path to the image file\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            # Load the image using OpenCV\n",
    "            img = cv2.imread(img_path)\n",
    "            # Convert the image to grayscale\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # Normalize the pixel values of the images\n",
    "            img = img.astype('float32') / 255.0\n",
    "            # Append the image to the images list\n",
    "            images.append(img)\n",
    "            # Append the label to the labels list\n",
    "            labels.append(folder)\n",
    "\n",
    "    # Convert the images and labels lists to NumPy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Return the images and labels arrays\n",
    "    return images, labels\n",
    "\n",
    "# Load the train images and labels\n",
    "train_images, train_labels = load_images(train_path)\n",
    "\n",
    "# Load the test images and labels\n",
    "test_images, test_labels = load_images(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78200, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78200,)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character_10_yna\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character_10_yna\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert the string labels to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_int = label_encoder.fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 occurs 1700 times\n",
      "1 occurs 1700 times\n",
      "2 occurs 1700 times\n",
      "3 occurs 1700 times\n",
      "4 occurs 1700 times\n",
      "5 occurs 1700 times\n",
      "6 occurs 1700 times\n",
      "7 occurs 1700 times\n",
      "8 occurs 1700 times\n",
      "9 occurs 1700 times\n",
      "10 occurs 1700 times\n",
      "11 occurs 1700 times\n",
      "12 occurs 1700 times\n",
      "13 occurs 1700 times\n",
      "14 occurs 1700 times\n",
      "15 occurs 1700 times\n",
      "16 occurs 1700 times\n",
      "17 occurs 1700 times\n",
      "18 occurs 1700 times\n",
      "19 occurs 1700 times\n",
      "20 occurs 1700 times\n",
      "21 occurs 1700 times\n",
      "22 occurs 1700 times\n",
      "23 occurs 1700 times\n",
      "24 occurs 1700 times\n",
      "25 occurs 1700 times\n",
      "26 occurs 1700 times\n",
      "27 occurs 1700 times\n",
      "28 occurs 1700 times\n",
      "29 occurs 1700 times\n",
      "30 occurs 1700 times\n",
      "31 occurs 1700 times\n",
      "32 occurs 1700 times\n",
      "33 occurs 1700 times\n",
      "34 occurs 1700 times\n",
      "35 occurs 1700 times\n",
      "36 occurs 1700 times\n",
      "37 occurs 1700 times\n",
      "38 occurs 1700 times\n",
      "39 occurs 1700 times\n",
      "40 occurs 1700 times\n",
      "41 occurs 1700 times\n",
      "42 occurs 1700 times\n",
      "43 occurs 1700 times\n",
      "44 occurs 1700 times\n",
      "45 occurs 1700 times\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(train_labels_int, return_counts=True)\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value} occurs {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the integer labels to one-hot encoding\n",
    "train_labels_onehot = to_categorical(train_labels_int, num_classes=46)\n",
    "\n",
    "# Split the data into the five quadrants\n",
    "x_train_tl = train_images[:,:14,:14].reshape(-1, 14, 14, 1)\n",
    "x_train_tr = train_images[:,:14,14:].reshape(-1, 14, 14, 1)\n",
    "x_train_bl = train_images[:,14:,:14].reshape(-1, 14, 14, 1)\n",
    "x_train_br = train_images[:,14:,14:].reshape(-1, 14, 14, 1)\n",
    "x_train_c = train_images[:,7:21,7:21].reshape(-1, 14, 14, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78200,)\n",
      "(78200, 46)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_labels_int.shape)\n",
    "print(train_labels_onehot.shape)\n",
    "print(train_labels_onehot[0])\n",
    "type(train_labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78200, 28, 28)\n",
      "(78200, 14, 14, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(x_train_tl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 12, 12, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 6, 6, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 2, 2, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 46)                23598     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,446\n",
      "Trainable params: 94,446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2200/2200 [==============================] - 16s 7ms/step - loss: 2.9265 - accuracy: 0.1888 - val_loss: 16.1239 - val_accuracy: 0.0683\n",
      "Epoch 2/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 2.1189 - accuracy: 0.3720 - val_loss: 16.7233 - val_accuracy: 0.0628\n",
      "Epoch 3/10\n",
      "2200/2200 [==============================] - 14s 7ms/step - loss: 1.8675 - accuracy: 0.4382 - val_loss: 18.3631 - val_accuracy: 0.0711\n",
      "Epoch 4/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.7131 - accuracy: 0.4791 - val_loss: 19.0629 - val_accuracy: 0.0900\n",
      "Epoch 5/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.6000 - accuracy: 0.5093 - val_loss: 20.6450 - val_accuracy: 0.0746\n",
      "Epoch 6/10\n",
      "2200/2200 [==============================] - 14s 6ms/step - loss: 1.5094 - accuracy: 0.5348 - val_loss: 21.0539 - val_accuracy: 0.0784\n",
      "Epoch 7/10\n",
      "2200/2200 [==============================] - 14s 6ms/step - loss: 1.4383 - accuracy: 0.5535 - val_loss: 21.7110 - val_accuracy: 0.0780\n",
      "Epoch 8/10\n",
      "2200/2200 [==============================] - 14s 6ms/step - loss: 1.3824 - accuracy: 0.5686 - val_loss: 21.3264 - val_accuracy: 0.0822\n",
      "Epoch 9/10\n",
      "2200/2200 [==============================] - 14s 6ms/step - loss: 1.3339 - accuracy: 0.5807 - val_loss: 21.4891 - val_accuracy: 0.0835\n",
      "Epoch 10/10\n",
      "2200/2200 [==============================] - 14s 7ms/step - loss: 1.2901 - accuracy: 0.5939 - val_loss: 22.1272 - val_accuracy: 0.0940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x195af4bba50>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN for top left quadrant\n",
    "\n",
    "# Define the input shape for each quadrant\n",
    "input_shape = (14, 14, 1)\n",
    "\n",
    "# Create a sequential model for the top left quadrant\n",
    "model_tl = Sequential()\n",
    "\n",
    "# Add a convolutional layer with 16 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_tl.add(Conv2D(16, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_tl.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a convolutional layer with 32 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_tl.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid'))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_tl.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output of the previous layer\n",
    "model_tl.add(Flatten())\n",
    "\n",
    "# Add a dense layer with 512 units and relu activation\n",
    "model_tl.add(Dense(512, activation='relu'))\n",
    "\n",
    "# Add the output layer with 46 units and softmax activation\n",
    "model_tl.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model_tl.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_tl.summary()\n",
    "\n",
    "# Train the model on the top left quadrant dataset\n",
    "model_tl.fit(x_train_tl, train_labels_onehot, batch_size=32, epochs=10, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 12, 12, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 6, 6, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 4, 4, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 2, 2, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 46)                23598     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,446\n",
      "Trainable params: 94,446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2200/2200 [==============================] - 16s 7ms/step - loss: 3.3063 - accuracy: 0.1183 - val_loss: 15.4203 - val_accuracy: 0.0377\n",
      "Epoch 2/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 2.5515 - accuracy: 0.2682 - val_loss: 17.3088 - val_accuracy: 0.0694\n",
      "Epoch 3/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 2.3062 - accuracy: 0.3300 - val_loss: 17.6611 - val_accuracy: 0.0831\n",
      "Epoch 4/10\n",
      "2200/2200 [==============================] - 16s 7ms/step - loss: 2.1722 - accuracy: 0.3648 - val_loss: 18.9922 - val_accuracy: 0.0921\n",
      "Epoch 5/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 2.0740 - accuracy: 0.3886 - val_loss: 19.7576 - val_accuracy: 0.0834\n",
      "Epoch 6/10\n",
      "2200/2200 [==============================] - 16s 7ms/step - loss: 1.9940 - accuracy: 0.4090 - val_loss: 19.6102 - val_accuracy: 0.0999\n",
      "Epoch 7/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.9303 - accuracy: 0.4253 - val_loss: 21.4739 - val_accuracy: 0.0723\n",
      "Epoch 8/10\n",
      "2200/2200 [==============================] - 14s 6ms/step - loss: 1.8749 - accuracy: 0.4422 - val_loss: 20.8903 - val_accuracy: 0.0900\n",
      "Epoch 9/10\n",
      "2200/2200 [==============================] - 19s 8ms/step - loss: 1.8284 - accuracy: 0.4516 - val_loss: 22.2905 - val_accuracy: 0.0719\n",
      "Epoch 10/10\n",
      "2200/2200 [==============================] - 22s 10ms/step - loss: 1.7878 - accuracy: 0.4644 - val_loss: 22.0322 - val_accuracy: 0.0802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x195b690f910>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN for top right quadrant\n",
    "\n",
    "# Define the input shape for each quadrant\n",
    "input_shape = (14, 14, 1)\n",
    "\n",
    "# Create a sequential model for the top left quadrant\n",
    "model_tr = Sequential()\n",
    "\n",
    "# Add a convolutional layer with 16 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_tr.add(Conv2D(16, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_tr.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a convolutional layer with 32 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_tr.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid'))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_tr.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output of the previous layer\n",
    "model_tr.add(Flatten())\n",
    "\n",
    "# Add a dense layer with 512 units and relu activation\n",
    "model_tr.add(Dense(512, activation='relu'))\n",
    "\n",
    "# Add the output layer with 46 units and softmax activation\n",
    "model_tr.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model_tr.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_tr.summary()\n",
    "\n",
    "# Train the model on the top left quadrant dataset\n",
    "model_tr.fit(x_train_tr, train_labels_onehot, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 12, 12, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 6, 6, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 4, 4, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 2, 2, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 46)                23598     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,446\n",
      "Trainable params: 94,446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2200/2200 [==============================] - 15s 6ms/step - loss: 2.8800 - accuracy: 0.2146 - val_loss: 17.6174 - val_accuracy: 0.0037\n",
      "Epoch 2/10\n",
      "2200/2200 [==============================] - 14s 7ms/step - loss: 1.8287 - accuracy: 0.4543 - val_loss: 19.5849 - val_accuracy: 0.0119\n",
      "Epoch 3/10\n",
      "2200/2200 [==============================] - 14s 6ms/step - loss: 1.5392 - accuracy: 0.5282 - val_loss: 21.1599 - val_accuracy: 0.0221\n",
      "Epoch 4/10\n",
      "2200/2200 [==============================] - 14s 7ms/step - loss: 1.3666 - accuracy: 0.5738 - val_loss: 22.6287 - val_accuracy: 0.0288\n",
      "Epoch 5/10\n",
      "2200/2200 [==============================] - 14s 7ms/step - loss: 1.2526 - accuracy: 0.6084 - val_loss: 23.5572 - val_accuracy: 0.0248\n",
      "Epoch 6/10\n",
      "2200/2200 [==============================] - 14s 7ms/step - loss: 1.1658 - accuracy: 0.6324 - val_loss: 23.5422 - val_accuracy: 0.0206\n",
      "Epoch 7/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.0973 - accuracy: 0.6518 - val_loss: 23.7382 - val_accuracy: 0.0263\n",
      "Epoch 8/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.0427 - accuracy: 0.6665 - val_loss: 23.8474 - val_accuracy: 0.0288\n",
      "Epoch 9/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 0.9961 - accuracy: 0.6794 - val_loss: 24.6470 - val_accuracy: 0.0276\n",
      "Epoch 10/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 0.9551 - accuracy: 0.6906 - val_loss: 25.0809 - val_accuracy: 0.0289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x195b6d87250>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN for bottom left quadrant\n",
    "\n",
    "# Define the input shape for each quadrant\n",
    "input_shape = (14, 14, 1)\n",
    "\n",
    "# Create a sequential model for the top left quadrant\n",
    "model_bl = Sequential()\n",
    "\n",
    "# Add a convolutional layer with 16 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_bl.add(Conv2D(16, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_bl.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a convolutional layer with 32 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_bl.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid'))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_bl.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output of the previous layer\n",
    "model_bl.add(Flatten())\n",
    "\n",
    "# Add a dense layer with 512 units and relu activation\n",
    "model_bl.add(Dense(512, activation='relu'))\n",
    "\n",
    "# Add the output layer with 46 units and softmax activation\n",
    "model_bl.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model_bl.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_bl.summary()\n",
    "\n",
    "# Train the model on the top left quadrant dataset\n",
    "model_bl.fit(x_train_bl, train_labels_onehot, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 12, 12, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 6, 6, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 4, 4, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 2, 2, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 46)                23598     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,446\n",
      "Trainable params: 94,446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 3.0108 - accuracy: 0.1720 - val_loss: 16.2951 - val_accuracy: 0.0244\n",
      "Epoch 2/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 2.1792 - accuracy: 0.3480 - val_loss: 18.1568 - val_accuracy: 0.0336\n",
      "Epoch 3/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.9552 - accuracy: 0.4035 - val_loss: 18.6313 - val_accuracy: 0.0569\n",
      "Epoch 4/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.8222 - accuracy: 0.4367 - val_loss: 21.1016 - val_accuracy: 0.0460\n",
      "Epoch 5/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.7262 - accuracy: 0.4632 - val_loss: 22.3318 - val_accuracy: 0.0435\n",
      "Epoch 6/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.6526 - accuracy: 0.4840 - val_loss: 23.6655 - val_accuracy: 0.0464\n",
      "Epoch 7/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.5965 - accuracy: 0.4964 - val_loss: 24.5354 - val_accuracy: 0.0517\n",
      "Epoch 8/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.5489 - accuracy: 0.5134 - val_loss: 24.9017 - val_accuracy: 0.0373\n",
      "Epoch 9/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.5057 - accuracy: 0.5251 - val_loss: 26.3957 - val_accuracy: 0.0381\n",
      "Epoch 10/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.4689 - accuracy: 0.5338 - val_loss: 26.4292 - val_accuracy: 0.0490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x195b6ad9590>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN for bottom right quadrant\n",
    "\n",
    "# Define the input shape for each quadrant\n",
    "input_shape = (14, 14, 1)\n",
    "\n",
    "# Create a sequential model for the top left quadrant\n",
    "model_br = Sequential()\n",
    "\n",
    "# Add a convolutional layer with 16 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_br.add(Conv2D(16, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_br.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a convolutional layer with 32 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_br.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid'))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_br.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output of the previous layer\n",
    "model_br.add(Flatten())\n",
    "\n",
    "# Add a dense layer with 512 units and relu activation\n",
    "model_br.add(Dense(512, activation='relu'))\n",
    "\n",
    "# Add the output layer with 46 units and softmax activation\n",
    "model_br.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model_br.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_br.summary()\n",
    "\n",
    "# Train the model on the top left quadrant dataset\n",
    "model_br.fit(x_train_br, train_labels_onehot, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 12, 12, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 6, 6, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 4, 4, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 2, 2, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 46)                23598     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,446\n",
      "Trainable params: 94,446\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2200/2200 [==============================] - 15s 6ms/step - loss: 2.9743 - accuracy: 0.2002 - val_loss: 18.3855 - val_accuracy: 0.0010\n",
      "Epoch 2/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.8881 - accuracy: 0.4531 - val_loss: 17.8526 - val_accuracy: 0.0242\n",
      "Epoch 3/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.5250 - accuracy: 0.5474 - val_loss: 18.7223 - val_accuracy: 0.0387\n",
      "Epoch 4/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.3112 - accuracy: 0.6042 - val_loss: 19.3821 - val_accuracy: 0.0251\n",
      "Epoch 5/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.1684 - accuracy: 0.6438 - val_loss: 19.7333 - val_accuracy: 0.0393\n",
      "Epoch 6/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 1.0620 - accuracy: 0.6708 - val_loss: 19.9941 - val_accuracy: 0.0473\n",
      "Epoch 7/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 0.9767 - accuracy: 0.6961 - val_loss: 21.0810 - val_accuracy: 0.0474\n",
      "Epoch 8/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 0.9121 - accuracy: 0.7123 - val_loss: 21.0437 - val_accuracy: 0.0471\n",
      "Epoch 9/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 0.8550 - accuracy: 0.7314 - val_loss: 21.3637 - val_accuracy: 0.0527\n",
      "Epoch 10/10\n",
      "2200/2200 [==============================] - 15s 7ms/step - loss: 0.8099 - accuracy: 0.7439 - val_loss: 21.7618 - val_accuracy: 0.0449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x195b6c1f9d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN for center quadrant\n",
    "\n",
    "# Define the input shape for each quadrant\n",
    "input_shape = (14, 14, 1)\n",
    "\n",
    "# Create a sequential model for the top left quadrant\n",
    "model_c = Sequential()\n",
    "\n",
    "# Add a convolutional layer with 16 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_c.add(Conv2D(16, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_c.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a convolutional layer with 32 filters, 3x3 kernel size, and sigmoid activation\n",
    "model_c.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid'))\n",
    "\n",
    "# Add a max pooling layer with 2x2 pool size\n",
    "model_c.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output of the previous layer\n",
    "model_c.add(Flatten())\n",
    "\n",
    "# Add a dense layer with 512 units and relu activation\n",
    "model_c.add(Dense(512, activation='relu'))\n",
    "\n",
    "# Add the output layer with 46 units and softmax activation\n",
    "model_c.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model_c.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_c.summary()\n",
    "\n",
    "# Train the model on the top left quadrant dataset\n",
    "model_c.fit(x_train_c, train_labels_onehot, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2444/2444 [==============================] - 8s 3ms/step\n",
      "2444/2444 [==============================] - 8s 3ms/step\n",
      "2444/2444 [==============================] - 8s 3ms/step\n",
      "2444/2444 [==============================] - 8s 3ms/step\n",
      "2444/2444 [==============================] - 8s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# create new models to extract features from the last dense layer\n",
    "model_tl_features = Model(inputs=model_tl.input, outputs=model_tl.get_layer('dense_10').output)\n",
    "model_tr_features = Model(inputs=model_tr.input, outputs=model_tr.get_layer('dense_12').output)\n",
    "model_bl_features = Model(inputs=model_bl.input, outputs=model_bl.get_layer('dense_14').output)\n",
    "model_br_features = Model(inputs=model_br.input, outputs=model_br.get_layer('dense_16').output)\n",
    "model_c_features = Model(inputs=model_c.input, outputs=model_c.get_layer('dense_18').output)\n",
    "\n",
    "# extract features from the last dense layer for each quadrant\n",
    "tl_features = model_tl_features.predict(x_train_tl)\n",
    "tr_features = model_tr_features.predict(x_train_tr)\n",
    "bl_features = model_bl_features.predict(x_train_bl)\n",
    "br_features = model_br_features.predict(x_train_br)\n",
    "center_features = model_c_features.predict(x_train_c)\n",
    "\n",
    "# concatenate the features from all 5 quadrants into a single input\n",
    "features = np.concatenate((tl_features, tr_features, bl_features, br_features, center_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_tl_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(model_tl_features\u001b[39m.\u001b[39msummary())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_tl_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(model_tl_features.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78200, 14, 14, 1)\n",
      "<keras.engine.functional.Functional object at 0x00000195B93FE350>\n",
      "(78200, 512)\n",
      "(78200, 2560)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tl.shape)\n",
    "print(model_tl_features)\n",
    "print(tl_features.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1955/1955 [==============================] - 561s 287ms/step - loss: 0.9240 - accuracy: 0.7403 - val_loss: 23.2201 - val_accuracy: 0.0212\n",
      "Epoch 2/10\n",
      "1955/1955 [==============================] - 458s 234ms/step - loss: 0.5411 - accuracy: 0.8641 - val_loss: 25.0291 - val_accuracy: 0.0215\n",
      "Epoch 3/10\n",
      "1955/1955 [==============================] - 372s 190ms/step - loss: 0.5124 - accuracy: 0.8801 - val_loss: 28.4862 - val_accuracy: 0.0214\n",
      "Epoch 4/10\n",
      "1955/1955 [==============================] - 329s 168ms/step - loss: 0.5118 - accuracy: 0.8855 - val_loss: 38.6992 - val_accuracy: 0.0213\n",
      "Epoch 5/10\n",
      "1955/1955 [==============================] - 272s 139ms/step - loss: 0.5221 - accuracy: 0.8865 - val_loss: 47.5726 - val_accuracy: 0.0215\n",
      "Epoch 6/10\n",
      "1955/1955 [==============================] - 285s 146ms/step - loss: 0.5388 - accuracy: 0.8876 - val_loss: 65.1303 - val_accuracy: 0.0214\n",
      "Epoch 7/10\n",
      "1955/1955 [==============================] - 284s 145ms/step - loss: 0.5425 - accuracy: 0.8885 - val_loss: 57.6047 - val_accuracy: 0.0217\n",
      "Epoch 8/10\n",
      "1955/1955 [==============================] - 289s 148ms/step - loss: 0.5382 - accuracy: 0.8895 - val_loss: 125.7249 - val_accuracy: 0.0216\n",
      "Epoch 9/10\n",
      "1955/1955 [==============================] - 286s 146ms/step - loss: 0.5704 - accuracy: 0.8882 - val_loss: 135.5056 - val_accuracy: 0.0217\n",
      "Epoch 10/10\n",
      "1955/1955 [==============================] - 285s 146ms/step - loss: 0.5968 - accuracy: 0.8881 - val_loss: 140.9781 - val_accuracy: 0.0216\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# define the DNN classifier model\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(2560, activation='relu', input_shape=(features.shape[1],)))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(1000, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(1000, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(1000, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(46, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = classifier.fit(features, train_labels_onehot, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the test data for prediction and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_int = label_encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the integer labels to one-hot encoding\n",
    "test_labels_onehot = to_categorical(test_labels_int, num_classes=46)\n",
    "\n",
    "# Split the data into the five quadrants\n",
    "x_test_tl = test_images[:,:14,:14].reshape(-1, 14, 14, 1)\n",
    "x_test_tr = test_images[:,:14,14:].reshape(-1, 14, 14, 1)\n",
    "x_test_bl = test_images[:,14:,:14].reshape(-1, 14, 14, 1)\n",
    "x_test_br = test_images[:,14:,14:].reshape(-1, 14, 14, 1)\n",
    "x_test_c = test_images[:,7:21,7:21].reshape(-1, 14, 14, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 1s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n",
      "432/432 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# extract features from the last dense layer for each quadrant\n",
    "tl_features_test = model_tl_features.predict(x_test_tl)\n",
    "tr_features_test = model_tr_features.predict(x_test_tr)\n",
    "bl_features_test = model_bl_features.predict(x_test_bl)\n",
    "br_features_test = model_br_features.predict(x_test_br)\n",
    "center_features_test = model_c_features.predict(x_test_c)\n",
    "\n",
    "# concatenate the features from all 5 quadrants into a single input\n",
    "features_test = np.concatenate((tl_features_test, tr_features_test, bl_features_test, br_features_test, center_features_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13800, 14, 14, 1)\n",
      "<keras.engine.functional.Functional object at 0x00000195B93FE350>\n",
      "(13800, 512)\n",
      "(13800, 2560)\n"
     ]
    }
   ],
   "source": [
    "print(x_test_tl.shape)\n",
    "print(model_tl_features)\n",
    "print(tl_features_test.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "classifier.save(\"dnn.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 7s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = classifier.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13800, 46)\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0000000e+00 0.0000000e+00 0.0000000e+00 7.1565388e-35 0.0000000e+00\n",
      " 4.0553214e-34 7.5576329e-34 9.4512729e-33 0.0000000e+00 3.3397569e-38\n",
      " 1.3861458e-29 6.3316524e-24 7.5105302e-26 7.2752837e-28 4.8854229e-34\n",
      " 1.6285259e-14 1.9164847e-17 1.1292561e-30 0.0000000e+00 7.5905522e-24\n",
      " 1.9051088e-37 3.2200462e-37 1.7306249e-27 1.6286748e-29 2.0155091e-20\n",
      " 0.0000000e+00 3.8113278e-24 9.5887772e-15 1.9335126e-18 1.9375273e-20\n",
      " 0.0000000e+00 3.4137172e-31 9.1156868e-27 0.0000000e+00 3.5423023e-10\n",
      " 6.9995083e-28 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = accuracy_score(test_labels_int, test_predictions.argmax(axis=1))\n",
    "precision = precision_score(test_labels_int, test_predictions.argmax(axis=1), average='weighted', zero_division=1)\n",
    "recall = recall_score(test_labels_int, test_predictions.argmax(axis=1), average='weighted')\n",
    "f1 = f1_score(test_labels_int, test_predictions.argmax(axis=1), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_labels_int[0])\n",
    "print(test_predictions.argmax(axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7683333333333333\n",
      "Precision: 0.8549186034032531\n",
      "Recall: 0.7683333333333333\n",
      "F1 Score: 0.6999743222420854\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume y_true and y_pred are the ground truth and predictions, respectively\n",
    "precision = precision_score(test_labels_int, test_predictions.argmax(axis=1), average='weighted', zero_division=0, labels=np.unique(test_predictions))\n",
    "\n",
    "# get the list of labels for which precision is being computed\n",
    "labels = np.unique(test_predictions.argmax(axis=1))\n",
    "\n",
    "# find the labels that were not predicted\n",
    "missing_labels = np.setdiff1d(np.unique(test_labels_int), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45]\n"
     ]
    }
   ],
   "source": [
    "print(missing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
